# PySpark Real Time Sceneraios

*  1.- Hot to create partitions based on year and month?
*  2.- How to handle or how to read variable/dinamic number of column of data file
*  3.- How to skip first few rows from data file
*  4.- How to handle remove duplicate records based on updated date
*  5.- How to get No of rows for each file
*  6.- how to get no of rows at each partition in DataFrame
*  7.- How to add a Sequence generated surrogate key as a column in dataframe
*  8.- How to get Individual column wise null records count / How to get No of null rows at each column from a dataframe
